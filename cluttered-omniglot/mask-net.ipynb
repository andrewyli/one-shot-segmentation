{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import os\n",
    "\n",
    "model_name = 'mask-net'\n",
    "DATASET_DIR = os.path.join(os.getcwd(), 'cluttered_omniglot/')\n",
    "LOG_DIR = os.path.join(os.getcwd(), 'logs/' + model_name + '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset: /home/andrewyli/cluttered-omniglot/cluttered_omniglot/8_characters/\n",
      "Done loading dataset\n",
      "Going to run for 20000 steps\n",
      "INFO:tensorflow:Restoring parameters from /home/andrewyli/cluttered-omniglot/logs/siamese-u-net/8_characters/Run.ckpt\n",
      "Starting to train\n"
     ]
    }
   ],
   "source": [
    "# Train encoder and discriminator\n",
    "# A trained Siamese-U-Net model is required\n",
    "number_of_characters = [16] # Or feed a list of clutter levels: [4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "for chars in range(len(number_of_characters)):\n",
    "    print('')\n",
    "    datadir = DATASET_DIR + '%.d_characters/'%(number_of_characters[chars])\n",
    "    logdir = LOG_DIR + 'encoder_decoder/%.d_characters/'%(number_of_characters[chars])\n",
    "    ckptdir = os.path.join(os.getcwd(), 'logs/siamese-u-net/') + '%.d_characters/'%(number_of_characters[chars])\n",
    "\n",
    "    model.training(datadir, \n",
    "             logdir, \n",
    "             epochs=5,\n",
    "             model=model_name,\n",
    "             train_mode='encoder_decoder',\n",
    "             feature_maps=24,\n",
    "             batch_size=50,\n",
    "             learning_rate=0.00005, \n",
    "             pretraining_checkpoint=ckptdir,\n",
    "             maximum_number_of_steps=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the discriminator\n",
    "number_of_characters = [16] # Or feed a list of clutter levels: [4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "for chars in range(len(number_of_characters)):\n",
    "    print('')\n",
    "    datadir = DATASET_DIR + '%.d_characters/'%(number_of_characters[chars])\n",
    "    logdir = LOG_DIR + 'discriminator/%.d_characters/'%(number_of_characters[chars])\n",
    "    ckptdir = LOG_DIR + 'encoder_decoder/%.d_characters/'%(number_of_characters[chars])\n",
    "\n",
    "    model.training(datadir, \n",
    "             logdir, \n",
    "             epochs=5,\n",
    "             model=model_name,\n",
    "             train_mode='discriminator',\n",
    "             feature_maps=24,\n",
    "             batch_size=250,\n",
    "             learning_rate=0.00025, \n",
    "             pretraining_checkpoint=ckptdir,\n",
    "             maximum_number_of_steps=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset: /home/andrewyli/one-shot-segmentation/cluttered-omniglot/cluttered_omniglot/16_characters/\n",
      "Loading test fold: /home/andrewyli/one-shot-segmentation/cluttered-omniglot/cluttered_omniglot/16_characters/fold_0000\n",
      "Loading test fold: /home/andrewyli/one-shot-segmentation/cluttered-omniglot/cluttered_omniglot/16_characters/fold_0000\n",
      "Done loading dataset\n",
      "INFO:tensorflow:Restoring parameters from /home/andrewyli/one-shot-segmentation/cluttered-omniglot/logs/mask-net/discriminator/16_characters/Run.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all clutter levels sequentially \n",
    "number_of_characters = [16]\n",
    "\n",
    "for chars in range(len(number_of_characters)):\n",
    "    print('')\n",
    "    datadir = DATASET_DIR + '%.d_characters/'%(number_of_characters[chars])\n",
    "    logdir = LOG_DIR + 'discriminator/%.d_characters/'%(number_of_characters[chars])\n",
    "\n",
    "    model.evaluation(datadir, \n",
    "             logdir, \n",
    "             model=model_name,\n",
    "             feature_maps=24,\n",
    "             batch_size=250,\n",
    "             threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
